{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domain Background\n",
    "\n",
    "After centuries of intense whaling, recovering whale populations still have a hard time adapting to warming oceans and struggle to compete every day with the industrial fishing industry for food.\n",
    "To aid whale conservation efforts, scientists use photo surveillance systems to monitor ocean activity. They use the shape of whales’ tails and unique markings found in footage to identify what species of whale they’re analyzing and meticulously log whale pod dynamics and movements. For the past 40 years, most of this work has been done manually by individual scientists, leaving a huge trove of data untapped and underutilized.\n",
    "There have been research done on identifying a whale using photos, which uses whale pictures which is similar to this effort, just that it uses the actual whale pictures.\n",
    "I chose this dataset as it looked interesting and it allowed me to focus more on the deep-learning techniques, and my interest in image classification.  The challenge is to identify individual whales in images given the image of its tail fin. We will analyze Happywhale’s database of over 25,000 images, gathered from research institutions and public contributors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9WtesfTyQBU3"
   },
   "source": [
    "# Download the kaggle data set.\n",
    "\n",
    "Download the dataset from Kaggle, by following these steps to install [Kaggle API]: https://github.com/Kaggle/kaggle-api.  Once the kaggle api is installed, do the following.\n",
    "\n",
    "* cd humpback-whale\n",
    "* kaggle competitions download -c humpback-whale-identification\n",
    "* mkdir -p input\n",
    "* unzip -d input/train train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "fHpFPr0BgRgi",
    "outputId": "481f6cf6-4a47-4499-d127-d153dfa9137d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "import gc\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mplimg\n",
    "from matplotlib.pyplot import imshow\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from keras import layers\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import image\n",
    "from keras.applications import VGG16, ResNet50, InceptionV3, Xception\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from keras.layers import Input, Dense, Activation, BatchNormalization, Flatten, Conv2D\n",
    "from keras.layers import AveragePooling2D, MaxPooling2D, Dropout, GlobalAveragePooling2D\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YF8I-wjLONtZ"
   },
   "source": [
    "# Explore the training data.\n",
    "In this kaggle dataset, it has train.csv, train.zip and test.zip.  But for the scope of this project, I will not be using the test.zip (as there is no way to validate the results).  I will use the train.csv and train.zip (train folder after extracting the archive to a folder) and split it into training and test datasets.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "yh6_zz3XOaSY",
    "outputId": "f8f64e6c-1504-459a-8789-01c17a37a2d4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image</th>\n",
       "      <th>Id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000e88ab.jpg</td>\n",
       "      <td>w_f48451c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0001f9222.jpg</td>\n",
       "      <td>w_c3d896a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00029d126.jpg</td>\n",
       "      <td>w_20df2c5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00050a15a.jpg</td>\n",
       "      <td>new_whale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0005c1ef8.jpg</td>\n",
       "      <td>new_whale</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Image         Id\n",
       "0  0000e88ab.jpg  w_f48451c\n",
       "1  0001f9222.jpg  w_c3d896a\n",
       "2  00029d126.jpg  w_20df2c5\n",
       "3  00050a15a.jpg  new_whale\n",
       "4  0005c1ef8.jpg  new_whale"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"train.csv\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "E3bAW_bFTBax",
    "outputId": "28bdf684-507b-4c81-c412-a6354c0cfc9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in train.csv 25361\n"
     ]
    }
   ],
   "source": [
    "print('Number of rows in train.csv', len(train_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F7vOENFsOhBP"
   },
   "source": [
    "# Identify the data points\n",
    "\n",
    "## train.csv\n",
    "There are 25361 rows in train.csv.  Which corresponds to the image entries in train.zip\n",
    "We can see that the train.csv file has two data fields.  \n",
    "* Image : The whale image file name\n",
    "* Id is the whale Id.\n",
    "Each whale is assigned a unique Id.  The unidentified whale's are assigned an Id new_whale.  \n",
    "\n",
    "\n",
    "## train.zip\n",
    "There are 25361 image files in train.zip file.  It has been extracted to input/train folder.  The filename corresponds to the Image column in train.csv file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique whales 5005\n"
     ]
    }
   ],
   "source": [
    "labels = train_df.Id\n",
    "\n",
    "# Encode labels to integers using sklearning.preprocessing.LabelEncoder\n",
    "# Convert the integer encoded array to category\n",
    "le = LabelEncoder()\n",
    "le.fit(labels)\n",
    "# Number of unique labels.\n",
    "num_classes = len(labels.value_counts())\n",
    "print('Number of unique whales {}'.format(len(le.classes_)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode the labels into categorical value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_transform = np_utils.to_categorical(le.transform(labels), num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MHJZxvxFdnAQ"
   },
   "source": [
    "## Split the data into training, validation & test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "8P63g4o2eCsV",
    "outputId": "a7f9ccbf-e673-4119-f9a4-92b2d5a3fdc9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training, Validation & testing data size 20288 2536 2537\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_tmp, Y_train, Y_tmp = train_test_split(train_df, y_transform, test_size=0.2, random_state=5)\n",
    "\n",
    "X_val, X_test, Y_val, Y_test   = train_test_split(X_tmp, Y_tmp, test_size=0.5, random_state=5)\n",
    "\n",
    "print('Training, Validation & testing data size', len(X_train),len(X_val), len(X_test))\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_height=100\n",
    "image_width=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ra5YW9NSxmyF"
   },
   "outputs": [],
   "source": [
    "def prepare_images(data):\n",
    "    print(\"Preparing images\")\n",
    "    images = np.zeros((len(data),image_height , image_width, 3))\n",
    "    count = 0\n",
    "    \n",
    "    for fig in data.Image:\n",
    "        #load images into images of size 100X100X3\n",
    "        img = image.load_img(\"input/train/\"+fig, target_size=(image_height, image_width, 3))\n",
    "        x = image.img_to_array(img)\n",
    "        x = preprocess_input(x)\n",
    "        images[count] = x\n",
    "        if (count%500 == 0):\n",
    "            print(\"Processing image: \", count+1, \", \", fig)\n",
    "        count += 1\n",
    "    count = 0\n",
    "    print(\"Finished!\")      \n",
    "    return images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9Bqz3rNkvKvf"
   },
   "source": [
    "# Create a CNN to create a base line model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 734
    },
    "colab_type": "code",
    "id": "zLNl8gYrcbtY",
    "outputId": "35599de9-473e-4659-e3c6-f18874826fdb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 100, 100, 16)      448       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 33, 33, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 33, 33, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 11, 11, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 11, 11, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 3, 3, 64)          0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_2 ( (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5005)              325325    \n",
      "=================================================================\n",
      "Total params: 348,909\n",
      "Trainable params: 348,909\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters = 16, kernel_size = 3, padding = 'same', activation = 'relu', \n",
    "          input_shape = (image_height, image_width, 3))) #RGB image\n",
    "model.add(MaxPooling2D(pool_size=3))\n",
    "\n",
    "model.add(Conv2D(filters = 32, kernel_size = 3,  padding = 'same', activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size=3))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(filters = 64, kernel_size = 3, padding = 'same', activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size=3))\n",
    "model.add(GlobalAveragePooling2D())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "8bHgwkdQqMNx",
    "outputId": "b654f6a9-f00f-47fb-dadd-152966b342fb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0719 23:29:22.744689 4641469888 deprecation_wrapper.py:119] From /anaconda3/envs/ml-project/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0719 23:29:22.762853 4641469888 deprecation_wrapper.py:119] From /anaconda3/envs/ml-project/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "czcKfy5IjWqS",
    "outputId": "ecbf6f42-e603-4589-e533-06c8fe08afb4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing images\n",
      "Processing image:  1 ,  5e2572252.jpg\n",
      "Processing image:  501 ,  b728ef1e9.jpg\n",
      "Processing image:  1001 ,  942ab5de3.jpg\n",
      "Processing image:  1501 ,  dd4cfa29f.jpg\n",
      "Processing image:  2001 ,  614f10ee7.jpg\n",
      "Processing image:  2501 ,  db9667359.jpg\n",
      "Processing image:  3001 ,  86c9aa515.jpg\n",
      "Processing image:  3501 ,  7f3aafbd2.jpg\n",
      "Processing image:  4001 ,  6f0c3deb4.jpg\n",
      "Processing image:  4501 ,  444b09aca.jpg\n",
      "Processing image:  5001 ,  f532c9318.jpg\n",
      "Processing image:  5501 ,  f2d3d0d0f.jpg\n",
      "Processing image:  6001 ,  6ca37fe7c.jpg\n",
      "Processing image:  6501 ,  3394e12db.jpg\n",
      "Processing image:  7001 ,  feddb3aa9.jpg\n",
      "Processing image:  7501 ,  3a8173905.jpg\n",
      "Processing image:  8001 ,  16ddf58df.jpg\n",
      "Processing image:  8501 ,  64b519010.jpg\n",
      "Processing image:  9001 ,  c2a02f80e.jpg\n",
      "Processing image:  9501 ,  770cb755e.jpg\n",
      "Processing image:  10001 ,  803515118.jpg\n",
      "Processing image:  10501 ,  5e8632b10.jpg\n",
      "Processing image:  11001 ,  5f37d323c.jpg\n",
      "Processing image:  11501 ,  204823b38.jpg\n",
      "Processing image:  12001 ,  27fdfe88c.jpg\n",
      "Processing image:  12501 ,  2272e1d48.jpg\n",
      "Processing image:  13001 ,  a7505ae38.jpg\n",
      "Processing image:  13501 ,  6faef4f7b.jpg\n",
      "Processing image:  14001 ,  788a2531c.jpg\n",
      "Processing image:  14501 ,  4e8713f2d.jpg\n",
      "Processing image:  15001 ,  a61a7cbf1.jpg\n",
      "Processing image:  15501 ,  f0109bc35.jpg\n",
      "Processing image:  16001 ,  f842d2d41.jpg\n",
      "Processing image:  16501 ,  1973d2873.jpg\n",
      "Processing image:  17001 ,  38b192e64.jpg\n",
      "Processing image:  17501 ,  b69a3106c.jpg\n",
      "Processing image:  18001 ,  898201c7f.jpg\n",
      "Processing image:  18501 ,  065fbc00f.jpg\n",
      "Processing image:  19001 ,  f5cfbf2df.jpg\n",
      "Processing image:  19501 ,  adaa80347.jpg\n",
      "Processing image:  20001 ,  a3785aebd.jpg\n",
      "Finished!\n",
      "Shape X-train:  (20288, 100, 100, 3)\n",
      "Preparing images\n",
      "Processing image:  1 ,  f6992fe3c.jpg\n",
      "Processing image:  501 ,  3c9ccb9b5.jpg\n",
      "Processing image:  1001 ,  62a96dde1.jpg\n",
      "Processing image:  1501 ,  f7a34b30e.jpg\n",
      "Processing image:  2001 ,  03e3599f3.jpg\n",
      "Processing image:  2501 ,  b9c0dba6f.jpg\n",
      "Finished!\n",
      "Shape X-val:  (2536, 100, 100, 3)\n",
      "Preparing images\n",
      "Processing image:  1 ,  448ae30f9.jpg\n",
      "Processing image:  501 ,  88766032d.jpg\n",
      "Processing image:  1001 ,  d5258d97c.jpg\n",
      "Processing image:  1501 ,  2e0f1128d.jpg\n",
      "Processing image:  2001 ,  4f72b1b40.jpg\n",
      "Processing image:  2501 ,  60bee807a.jpg\n",
      "Finished!\n",
      "Shape X-test:  (2537, 100, 100, 3)\n"
     ]
    }
   ],
   "source": [
    "from PIL import ImageFile                            \n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True \n",
    "\n",
    "x_train_images = prepare_images(X_train)\n",
    "x_train_images /= 255\n",
    "\n",
    "print(\"Shape X-train: \", x_train_images.shape)\n",
    "\n",
    "x_val_images = prepare_images(X_val)\n",
    "x_val_images /= 255\n",
    "\n",
    "print(\"Shape X-val: \", x_val_images.shape)\n",
    "\n",
    "x_test_images = prepare_images(X_test)\n",
    "x_test_images /= 255\n",
    "\n",
    "print(\"Shape X-test: \", x_test_images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "z1RdJJQdkn0U",
    "outputId": "7578f359-1296-407e-8abc-89c7c88929a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20288 samples, validate on 2536 samples\n",
      "Epoch 1/50\n",
      "20288/20288 [==============================] - 43s 2ms/step - loss: 5.6427 - acc: 0.3815 - val_loss: 5.8940 - val_acc: 0.3825\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 5.89397, saving model to saved_models/weight.best.from_scratch.hdf5\n",
      "Epoch 2/50\n",
      "20288/20288 [==============================] - 43s 2ms/step - loss: 5.6327 - acc: 0.3815 - val_loss: 5.9505 - val_acc: 0.3825\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 5.89397\n",
      "Epoch 3/50\n",
      "20288/20288 [==============================] - 43s 2ms/step - loss: 5.6246 - acc: 0.3815 - val_loss: 5.8955 - val_acc: 0.3825\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 5.89397\n",
      "Epoch 4/50\n",
      "20288/20288 [==============================] - 44s 2ms/step - loss: 5.6195 - acc: 0.3815 - val_loss: 5.8859 - val_acc: 0.3825\n",
      "\n",
      "Epoch 00004: val_loss improved from 5.89397 to 5.88593, saving model to saved_models/weight.best.from_scratch.hdf5\n",
      "Epoch 5/50\n",
      "20288/20288 [==============================] - 44s 2ms/step - loss: 5.5981 - acc: 0.3815 - val_loss: 5.8892 - val_acc: 0.3825\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 5.88593\n",
      "Epoch 6/50\n",
      "20288/20288 [==============================] - 43s 2ms/step - loss: 5.5890 - acc: 0.3815 - val_loss: 5.8510 - val_acc: 0.3825\n",
      "\n",
      "Epoch 00006: val_loss improved from 5.88593 to 5.85100, saving model to saved_models/weight.best.from_scratch.hdf5\n",
      "Epoch 7/50\n",
      "20288/20288 [==============================] - 43s 2ms/step - loss: 5.5783 - acc: 0.3815 - val_loss: 5.8866 - val_acc: 0.3825\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 5.85100\n",
      "Epoch 8/50\n",
      "20288/20288 [==============================] - 43s 2ms/step - loss: 5.5766 - acc: 0.3815 - val_loss: 5.9147 - val_acc: 0.3825\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 5.85100\n",
      "Epoch 9/50\n",
      "20288/20288 [==============================] - 45s 2ms/step - loss: 5.5730 - acc: 0.3815 - val_loss: 5.8478 - val_acc: 0.3825\n",
      "\n",
      "Epoch 00009: val_loss improved from 5.85100 to 5.84784, saving model to saved_models/weight.best.from_scratch.hdf5\n",
      "Epoch 10/50\n",
      "20288/20288 [==============================] - 43s 2ms/step - loss: 5.5648 - acc: 0.3815 - val_loss: 5.8946 - val_acc: 0.3825\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 5.84784\n",
      "Epoch 11/50\n",
      "20288/20288 [==============================] - 45s 2ms/step - loss: 5.5521 - acc: 0.3815 - val_loss: 5.8707 - val_acc: 0.3825\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 5.84784\n",
      "Epoch 12/50\n",
      "20288/20288 [==============================] - 43s 2ms/step - loss: 5.5471 - acc: 0.3815 - val_loss: 5.8753 - val_acc: 0.3825\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 5.84784\n",
      "Epoch 13/50\n",
      "20288/20288 [==============================] - 43s 2ms/step - loss: 5.5369 - acc: 0.3815 - val_loss: 5.9094 - val_acc: 0.3825\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 5.84784\n",
      "Epoch 14/50\n",
      "20288/20288 [==============================] - 42s 2ms/step - loss: 5.5328 - acc: 0.3815 - val_loss: 5.8307 - val_acc: 0.3825\n",
      "\n",
      "Epoch 00014: val_loss improved from 5.84784 to 5.83068, saving model to saved_models/weight.best.from_scratch.hdf5\n",
      "Epoch 15/50\n",
      "20288/20288 [==============================] - 43s 2ms/step - loss: 5.5293 - acc: 0.3815 - val_loss: 5.8644 - val_acc: 0.3825\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 5.83068\n",
      "Epoch 16/50\n",
      "20288/20288 [==============================] - 43s 2ms/step - loss: 5.5210 - acc: 0.3815 - val_loss: 5.8497 - val_acc: 0.3825\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 5.83068\n",
      "Epoch 17/50\n",
      "20288/20288 [==============================] - 43s 2ms/step - loss: 5.5157 - acc: 0.3815 - val_loss: 5.8497 - val_acc: 0.3825\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 5.83068\n",
      "Epoch 18/50\n",
      "20288/20288 [==============================] - 43s 2ms/step - loss: 5.5111 - acc: 0.3815 - val_loss: 5.8869 - val_acc: 0.3825\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 5.83068\n",
      "Epoch 19/50\n",
      "20288/20288 [==============================] - 43s 2ms/step - loss: 5.5010 - acc: 0.3815 - val_loss: 5.8613 - val_acc: 0.3825\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 5.83068\n",
      "Epoch 20/50\n",
      "20288/20288 [==============================] - 43s 2ms/step - loss: 5.4970 - acc: 0.3815 - val_loss: 5.8571 - val_acc: 0.3825\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 5.83068\n",
      "Epoch 21/50\n",
      " 3700/20288 [====>.........................] - ETA: 32s - loss: 5.4914 - acc: 0.3808"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "os.makedirs('saved_models', exist_ok=True)\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weight.best.from_scratch.hdf5',\n",
    "                               verbose=1, save_best_only = True)\n",
    "model.fit(x_train_images, Y_train, epochs=50, batch_size=100, verbose=1,\n",
    "                   validation_data=(x_val_images, Y_val), callbacks=[checkpointer])\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "rGvw1d3YmeYf",
    "outputId": "7156dac5-7e99-4251-fd8d-1e2c5fbdcc9f"
   },
   "outputs": [],
   "source": [
    "model.load_weights('saved_models/weight.best.from_scratch.hdf5')\n",
    "pred = model.predict(x_test_images, verbose=1)\n",
    "print(pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3JuQhrdf9wa8"
   },
   "source": [
    "## MAP@5 for Base CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "_RTx0Sfs37Gk",
    "outputId": "eeb010c4-59d7-4e80-bbaf-97cc3bc25062"
   },
   "outputs": [],
   "source": [
    "import map5_score as map5\n",
    "predictions=[le.inverse_transform(p.argsort()[-5:][::-1]) for i, p in enumerate(pred) ]\n",
    "# for i, p in enumerate(pred):\n",
    "#     predictions.append(le.inverse_transform(p.argsort()[-5:][::-1]).tolist())\n",
    "print('MAP@5 score for Base model = %.5f' %(map5.map5_per_set(X_test.Id, predictions )))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yYw1BWYVJiZi"
   },
   "source": [
    "# Using VGG19 and Transfer learning\n",
    "\n",
    "To reduce training time without sacrificing accuracy, lets train a CNN using transfer learning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_steps_size(generator, batch_size):\n",
    "    nb_samples = len(generator.filenames) \n",
    "    return int(math.ceil(nb_samples / batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wulZQnv2KHfN"
   },
   "outputs": [],
   "source": [
    "vgg_model = ResNet50(include_top=False, weights='imagenet', pooling='max')\n",
    "vgg_model.summary()\n",
    "train_datagen = image.ImageDataGenerator(\n",
    "    rescale=1./255, rotation_range=40, \n",
    "    horizontal_flip=True, zoom_range=0.2, \n",
    "    shear_range=0.2, fill_mode='nearest')\n",
    "batch_size=100\n",
    "\n",
    "generator = train_datagen.flow_from_dataframe(dataframe=X_train,\n",
    "                                              directory='input/train',\n",
    "                                              x_col=\"Image\",\n",
    "                                              y_col=\"Id\", \n",
    "                                              classes=le.classes_.tolist(),\n",
    "                                              target_size=(image_height, image_width),\n",
    "                                              batch_size=batch_size, shuffle=False, drop_duplicates=False) \n",
    "\n",
    "\n",
    "train_bc_features = vgg_model.predict_generator(generator, steps=get_steps_size(generator, batch_size),verbose=1)\n",
    "\n",
    "test_datagen = image.ImageDataGenerator(rescale=1./255)\n",
    "generator = test_datagen.flow_from_dataframe(dataframe=X_val,\n",
    "                                              directory='input/train',\n",
    "                                              x_col=\"Image\",\n",
    "                                              y_col=\"Id\", \n",
    "                                              classes=le.classes_.tolist(),\n",
    "                                              target_size=(image_height, image_width),\n",
    "                                              batch_size=batch_size,\n",
    "                                              shuffle=False,\n",
    "                                              drop_duplicates=False)  \n",
    "val_bc_features = vgg_model.predict_generator(generator, steps=get_steps_size(generator, batch_size),verbose=1)\n",
    "\n",
    "generator = test_datagen.flow_from_dataframe(dataframe=X_test,\n",
    "                                              directory='input/train',\n",
    "                                              x_col=\"Image\",\n",
    "                                              y_col=\"Id\", \n",
    "                                              classes=le.classes_.tolist(),\n",
    "                                              target_size=(image_height, image_width),\n",
    "                                              batch_size=batch_size,\n",
    "                                              shuffle=False,\n",
    "                                              drop_duplicates=False)  \n",
    "test_bc_features = vgg_model.predict_generator(generator, steps=get_steps_size(generator, batch_size),verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=train_bc_features.shape[1:]))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',  \n",
    "              loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint(filepath='saved_models/bottleneck_fc_model.hdf5',\n",
    "                               verbose=1, save_best_only = True)\n",
    "\n",
    "history = model.fit(train_bc_features, Y_train,  \n",
    "          epochs=15,  \n",
    "          batch_size=batch_size,  \n",
    "          validation_data=(val_bc_features, Y_val), callbacks=[checkpointer])  \n",
    "   \n",
    "(eval_loss, eval_accuracy) = model.evaluate(  \n",
    "     val_bc_features, Y_val, batch_size=batch_size, verbose=1)\n",
    "\n",
    "print(\"[INFO] accuracy: {:.2f}%\".format(eval_accuracy * 100))  \n",
    "print(\"[INFO] Loss: {}\".format(eval_loss))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('saved_models/bottleneck_fc_model.hdf5')\n",
    "pred = model.predict(test_bc_features, verbose=1)\n",
    "\n",
    "predictions=[]\n",
    "for i, p in enumerate(pred):\n",
    "    predictions.append(le.inverse_transform(p.argsort()[-5:][::-1]).tolist())\n",
    "print('MAP@5 score for VGG16 model = %.5f' %(map5.map5_per_set(X_test.Id, predictions )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "udacity_capstone.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
