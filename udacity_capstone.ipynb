{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "udacity_capstone.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vrvarma/humpback-whale/blob/master/udacity_capstone.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ghtNPB0iWKM",
        "colab_type": "text"
      },
      "source": [
        "# Domain Background\n",
        "\n",
        "After centuries of intense whaling, recovering whale populations still have a hard time adapting to warming oceans and struggle to compete every day with the industrial fishing industry for food.\n",
        "To aid whale conservation efforts, scientists use photo surveillance systems to monitor ocean activity. They use the shape of whales’ tails and unique markings found in footage to identify what species of whale they’re analyzing and meticulously log whale pod dynamics and movements. For the past 40 years, most of this work has been done manually by individual scientists, leaving a huge trove of data untapped and underutilized.\n",
        "There have been research done on identifying a whale using photos, which uses whale pictures which is similar to this effort, just that it uses the actual whale pictures.\n",
        "I chose this dataset as it looked interesting and it allowed me to focus more on the deep-learning techniques, and my interest in image classification.  The challenge is to identify individual whales in images given the image of its tail fin. We will analyze Happywhale’s database of over 25,000 images, gathered from research institutions and public contributors.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9WtesfTyQBU3"
      },
      "source": [
        "# Download the kaggle data set.\n",
        "\n",
        "Download the dataset from Kaggle, by following these steps to install [Kaggle API]: https://github.com/Kaggle/kaggle-api.  Once the kaggle api is installed, do the following.\n",
        "\n",
        "* cd humpback-whale\n",
        "* kaggle competitions download -c humpback-whale-identification\n",
        "* mkdir -p input\n",
        "* unzip -d input/train train.zip"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGV91slxinCU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !mkdir -p ~/.kaggle\n",
        "# !cp kaggle.json ~/.kaggle\n",
        "# !chmod 600 ~/.kaggle\n",
        "# !kaggle competitions download -c humpback-whale-identification \n",
        "# !mkdir -p input \n",
        "# !unzip -q -d input/train train.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGjria3miAm1",
        "colab_type": "text"
      },
      "source": [
        "# Import python packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fHpFPr0BgRgi",
        "colab": {}
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "import os\n",
        "import gc\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mplimg\n",
        "from matplotlib.pyplot import imshow\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "from keras import layers\n",
        "from keras.utils import np_utils\n",
        "from keras.preprocessing import image\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.applications import ResNet50, InceptionV3, Xception,VGG19\n",
        "from keras.applications.imagenet_utils import preprocess_input\n",
        "from keras.layers import Input, Dense, Activation, BatchNormalization, Flatten, Conv2D\n",
        "from keras.layers import AveragePooling2D, MaxPooling2D, Dropout, GlobalAveragePooling2D\n",
        "from keras.models import Model\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "import keras.backend as K\n",
        "from keras.models import Sequential\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\", category=DeprecationWarning)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YF8I-wjLONtZ"
      },
      "source": [
        "# Explore the training data.\n",
        "In this kaggle dataset, it has train.csv, train.zip and test.zip.  But for the scope of this project, I will not be using the test.zip (as there is no way to validate the results).  I will use the train.csv and train.zip (train folder after extracting the archive to a folder) and split it into training and test datasets.  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yh6_zz3XOaSY",
        "outputId": "7df876db-f05f-4278-94b7-ca05d2208764",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "train_df = pd.read_csv(\"train.csv\")\n",
        "train_df.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Image</th>\n",
              "      <th>Id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0000e88ab.jpg</td>\n",
              "      <td>w_f48451c</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0001f9222.jpg</td>\n",
              "      <td>w_c3d896a</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>00029d126.jpg</td>\n",
              "      <td>w_20df2c5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>00050a15a.jpg</td>\n",
              "      <td>new_whale</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0005c1ef8.jpg</td>\n",
              "      <td>new_whale</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           Image         Id\n",
              "0  0000e88ab.jpg  w_f48451c\n",
              "1  0001f9222.jpg  w_c3d896a\n",
              "2  00029d126.jpg  w_20df2c5\n",
              "3  00050a15a.jpg  new_whale\n",
              "4  0005c1ef8.jpg  new_whale"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "E3bAW_bFTBax",
        "outputId": "66a0fc24-806d-42f6-a7e8-2688a5db4a10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print('Number of rows in train.csv', len(train_df))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of rows in train.csv 25361\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "F7vOENFsOhBP"
      },
      "source": [
        "# Identify the data points\n",
        "\n",
        "## train.csv\n",
        "There are 25361 rows in train.csv.  Which corresponds to the image entries in train.zip\n",
        "We can see that the train.csv file has two data fields.  \n",
        "* Image : The whale image file name\n",
        "* Id is the whale Id.\n",
        "Each whale is assigned a unique Id.  The unidentified whale's are assigned an Id new_whale.  \n",
        "\n",
        "\n",
        "## train.zip\n",
        "There are 25361 image files in train.zip file.  It has been extracted to input/train folder.  The filename corresponds to the Image column in train.csv file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5DUcR50iWKa",
        "colab_type": "text"
      },
      "source": [
        "# Data pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XozWc-KOiWKb",
        "colab_type": "text"
      },
      "source": [
        "## Label Encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEBdbmzziWKb",
        "colab_type": "code",
        "outputId": "8e3292b3-52e4-48fe-dafd-9a87f1faefeb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "labels = train_df.Id\n",
        "\n",
        "# Encode labels to integers using sklearning.preprocessing.LabelEncoder\n",
        "# Convert the integer encoded array to category\n",
        "le = LabelEncoder()\n",
        "le.fit(labels)\n",
        "# Number of unique labels.\n",
        "num_classes = len(labels.value_counts())\n",
        "print('Number of unique whales {}'.format(len(le.classes_)))\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of unique whales 5005\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5FJOsHmiWKe",
        "colab_type": "text"
      },
      "source": [
        "## Encode the labels into categorical value"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5sqt0BliWKe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_transform = np_utils.to_categorical(le.transform(labels), num_classes=num_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MHJZxvxFdnAQ"
      },
      "source": [
        "## Split the data into training, validation & test datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8P63g4o2eCsV",
        "outputId": "ece6513d-859e-4bac-f62c-6fdf499e0927",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "X_train, X_tmp, Y_train, Y_tmp = train_test_split(train_df, y_transform, test_size=0.2, random_state=5)\n",
        "\n",
        "X_val, X_test, Y_val, Y_test   = train_test_split(X_tmp, Y_tmp, test_size=0.5, random_state=5)\n",
        "\n",
        "print('Training, Validation & testing data size', len(X_train),len(X_val), len(X_test))\n",
        "gc.collect()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training, Validation & testing data size 20288 2536 2537\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdheLP1giWKl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image_height=100\n",
        "image_width=100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ra5YW9NSxmyF",
        "colab": {}
      },
      "source": [
        "def prepare_images(data):\n",
        "    print(\"Preparing images\")\n",
        "    images = np.zeros((len(data),image_height , image_width, 3))\n",
        "    count = 0\n",
        "    \n",
        "    for fig in data.Image:\n",
        "        #load images into images of size 100X100X3\n",
        "        img = image.load_img(\"input/train/\"+fig, target_size=(image_height, image_width, 3))\n",
        "        x = image.img_to_array(img)\n",
        "        x = preprocess_input(x)\n",
        "        images[count] = x\n",
        "        if (count%500 == 0):\n",
        "            print(\"Processing image: \", count+1, \", \", fig)\n",
        "        count += 1\n",
        "    count = 0\n",
        "    print(\"Finished!\")      \n",
        "    return images"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9Bqz3rNkvKvf"
      },
      "source": [
        "# Create a CNN to create a base line model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ify-r3SEiWKp",
        "colab_type": "text"
      },
      "source": [
        "## Build Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zLNl8gYrcbtY",
        "outputId": "52e1d7b1-ae92-4446-9830-0f65571db68c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 734
        }
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(filters = 16, kernel_size = 3, padding = 'same', activation = 'relu', \n",
        "          input_shape = (image_height, image_width, 3))) #RGB image\n",
        "model.add(MaxPooling2D(pool_size=3))\n",
        "\n",
        "model.add(Conv2D(filters = 32, kernel_size = 3,  padding = 'same', activation = 'relu'))\n",
        "model.add(MaxPooling2D(pool_size=3))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(filters = 64, kernel_size = 3, padding = 'same', activation = 'relu'))\n",
        "model.add(MaxPooling2D(pool_size=3))\n",
        "model.add(GlobalAveragePooling2D())\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.summary()\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0720 08:06:36.843943 140085875619712 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0720 08:06:36.859598 140085875619712 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0720 08:06:36.862548 140085875619712 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0720 08:06:36.876597 140085875619712 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "W0720 08:06:36.891286 140085875619712 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0720 08:06:36.898467 140085875619712 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 100, 100, 16)      448       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 33, 33, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 33, 33, 32)        4640      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 11, 11, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 11, 11, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 11, 11, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 3, 3, 64)          0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_1 ( (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 5005)              325325    \n",
            "=================================================================\n",
            "Total params: 348,909\n",
            "Trainable params: 348,909\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PuA6CTSoiWKs",
        "colab_type": "text"
      },
      "source": [
        "## Compile Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8bHgwkdQqMNx",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import adam\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lppC13PSiWKv",
        "colab_type": "text"
      },
      "source": [
        "## Prepare Images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "czcKfy5IjWqS",
        "outputId": "fb2bda39-ff28-4f32-82d8-1623f60a7fb3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from PIL import ImageFile                            \n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True \n",
        "\n",
        "x_train_images = prepare_images(X_train)\n",
        "x_train_images /= 255\n",
        "\n",
        "print(\"Shape X-train: \", x_train_images.shape)\n",
        "\n",
        "x_val_images = prepare_images(X_val)\n",
        "x_val_images /= 255\n",
        "\n",
        "print(\"Shape X-val: \", x_val_images.shape)\n",
        "\n",
        "x_test_images = prepare_images(X_test)\n",
        "x_test_images /= 255\n",
        "\n",
        "print(\"Shape X-test: \", x_test_images.shape)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Preparing images\n",
            "Processing image:  1 ,  5e2572252.jpg\n",
            "Processing image:  501 ,  b728ef1e9.jpg\n",
            "Processing image:  1001 ,  942ab5de3.jpg\n",
            "Processing image:  1501 ,  dd4cfa29f.jpg\n",
            "Processing image:  2001 ,  614f10ee7.jpg\n",
            "Processing image:  2501 ,  db9667359.jpg\n",
            "Processing image:  3001 ,  86c9aa515.jpg\n",
            "Processing image:  3501 ,  7f3aafbd2.jpg\n",
            "Processing image:  4001 ,  6f0c3deb4.jpg\n",
            "Processing image:  4501 ,  444b09aca.jpg\n",
            "Processing image:  5001 ,  f532c9318.jpg\n",
            "Processing image:  5501 ,  f2d3d0d0f.jpg\n",
            "Processing image:  6001 ,  6ca37fe7c.jpg\n",
            "Processing image:  6501 ,  3394e12db.jpg\n",
            "Processing image:  7001 ,  feddb3aa9.jpg\n",
            "Processing image:  7501 ,  3a8173905.jpg\n",
            "Processing image:  8001 ,  16ddf58df.jpg\n",
            "Processing image:  8501 ,  64b519010.jpg\n",
            "Processing image:  9001 ,  c2a02f80e.jpg\n",
            "Processing image:  9501 ,  770cb755e.jpg\n",
            "Processing image:  10001 ,  803515118.jpg\n",
            "Processing image:  10501 ,  5e8632b10.jpg\n",
            "Processing image:  11001 ,  5f37d323c.jpg\n",
            "Processing image:  11501 ,  204823b38.jpg\n",
            "Processing image:  12001 ,  27fdfe88c.jpg\n",
            "Processing image:  12501 ,  2272e1d48.jpg\n",
            "Processing image:  13001 ,  a7505ae38.jpg\n",
            "Processing image:  13501 ,  6faef4f7b.jpg\n",
            "Processing image:  14001 ,  788a2531c.jpg\n",
            "Processing image:  14501 ,  4e8713f2d.jpg\n",
            "Processing image:  15001 ,  a61a7cbf1.jpg\n",
            "Processing image:  15501 ,  f0109bc35.jpg\n",
            "Processing image:  16001 ,  f842d2d41.jpg\n",
            "Processing image:  16501 ,  1973d2873.jpg\n",
            "Processing image:  17001 ,  38b192e64.jpg\n",
            "Processing image:  17501 ,  b69a3106c.jpg\n",
            "Processing image:  18001 ,  898201c7f.jpg\n",
            "Processing image:  18501 ,  065fbc00f.jpg\n",
            "Processing image:  19001 ,  f5cfbf2df.jpg\n",
            "Processing image:  19501 ,  adaa80347.jpg\n",
            "Processing image:  20001 ,  a3785aebd.jpg\n",
            "Finished!\n",
            "Shape X-train:  (20288, 100, 100, 3)\n",
            "Preparing images\n",
            "Processing image:  1 ,  f6992fe3c.jpg\n",
            "Processing image:  501 ,  3c9ccb9b5.jpg\n",
            "Processing image:  1001 ,  62a96dde1.jpg\n",
            "Processing image:  1501 ,  f7a34b30e.jpg\n",
            "Processing image:  2001 ,  03e3599f3.jpg\n",
            "Processing image:  2501 ,  b9c0dba6f.jpg\n",
            "Finished!\n",
            "Shape X-val:  (2536, 100, 100, 3)\n",
            "Preparing images\n",
            "Processing image:  1 ,  448ae30f9.jpg\n",
            "Processing image:  501 ,  88766032d.jpg\n",
            "Processing image:  1001 ,  d5258d97c.jpg\n",
            "Processing image:  1501 ,  2e0f1128d.jpg\n",
            "Processing image:  2001 ,  4f72b1b40.jpg\n",
            "Processing image:  2501 ,  60bee807a.jpg\n",
            "Finished!\n",
            "Shape X-test:  (2537, 100, 100, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QteXGRkiWKz",
        "colab_type": "text"
      },
      "source": [
        "## Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87vW4MEhkTC-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.makedirs('saved_models', exist_ok=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0KXJE-03yjU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0129c171-4390-49c5-fa01-3ecc26be2454"
      },
      "source": [
        "gc.collect()"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "471"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "z1RdJJQdkn0U",
        "outputId": "1f1adfef-3ddf-457c-87c7-be6c5ed4ed4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "checkpointer = ModelCheckpoint(filepath='saved_models/weight.best.from_scratch.hdf5',\n",
        "                               verbose=1, save_best_only = True)\n",
        "model.fit(x_train_images, Y_train, epochs=20, batch_size=500, verbose=1,\n",
        "                   validation_data=(x_val_images, Y_val), callbacks=[checkpointer])\n",
        "gc.collect()\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0720 08:10:22.183230 140085875619712 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 20288 samples, validate on 2536 samples\n",
            "Epoch 1/20\n",
            "20288/20288 [==============================] - 11s 553us/step - loss: 7.2307 - categorical_accuracy: 0.3422 - val_loss: 6.1627 - val_categorical_accuracy: 0.3825\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 6.16271, saving model to saved_models/weight.best.from_scratch.hdf5\n",
            "Epoch 2/20\n",
            "20288/20288 [==============================] - 7s 325us/step - loss: 6.0330 - categorical_accuracy: 0.3815 - val_loss: 6.0763 - val_categorical_accuracy: 0.3825\n",
            "\n",
            "Epoch 00002: val_loss improved from 6.16271 to 6.07634, saving model to saved_models/weight.best.from_scratch.hdf5\n",
            "Epoch 3/20\n",
            "20288/20288 [==============================] - 7s 323us/step - loss: 5.9272 - categorical_accuracy: 0.3815 - val_loss: 6.1231 - val_categorical_accuracy: 0.3825\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 6.07634\n",
            "Epoch 4/20\n",
            "20288/20288 [==============================] - 6s 320us/step - loss: 5.8890 - categorical_accuracy: 0.3815 - val_loss: 6.1304 - val_categorical_accuracy: 0.3825\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 6.07634\n",
            "Epoch 5/20\n",
            "20288/20288 [==============================] - 6s 314us/step - loss: 5.8579 - categorical_accuracy: 0.3815 - val_loss: 6.0932 - val_categorical_accuracy: 0.3825\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 6.07634\n",
            "Epoch 6/20\n",
            "20288/20288 [==============================] - 7s 322us/step - loss: 5.8130 - categorical_accuracy: 0.3815 - val_loss: 6.0639 - val_categorical_accuracy: 0.3825\n",
            "\n",
            "Epoch 00006: val_loss improved from 6.07634 to 6.06392, saving model to saved_models/weight.best.from_scratch.hdf5\n",
            "Epoch 7/20\n",
            "20288/20288 [==============================] - 7s 330us/step - loss: 5.7967 - categorical_accuracy: 0.3815 - val_loss: 6.0888 - val_categorical_accuracy: 0.3825\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 6.06392\n",
            "Epoch 8/20\n",
            "20288/20288 [==============================] - 7s 334us/step - loss: 5.7686 - categorical_accuracy: 0.3815 - val_loss: 6.1372 - val_categorical_accuracy: 0.3825\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 6.06392\n",
            "Epoch 9/20\n",
            "20288/20288 [==============================] - 7s 330us/step - loss: 5.7624 - categorical_accuracy: 0.3815 - val_loss: 6.0615 - val_categorical_accuracy: 0.3825\n",
            "\n",
            "Epoch 00009: val_loss improved from 6.06392 to 6.06146, saving model to saved_models/weight.best.from_scratch.hdf5\n",
            "Epoch 10/20\n",
            "20288/20288 [==============================] - 7s 331us/step - loss: 5.7374 - categorical_accuracy: 0.3815 - val_loss: 6.0835 - val_categorical_accuracy: 0.3825\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 6.06146\n",
            "Epoch 11/20\n",
            "20288/20288 [==============================] - 7s 330us/step - loss: 5.7482 - categorical_accuracy: 0.3815 - val_loss: 6.0693 - val_categorical_accuracy: 0.3825\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 6.06146\n",
            "Epoch 12/20\n",
            "20288/20288 [==============================] - 7s 336us/step - loss: 5.7290 - categorical_accuracy: 0.3815 - val_loss: 6.0770 - val_categorical_accuracy: 0.3825\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 6.06146\n",
            "Epoch 13/20\n",
            "20288/20288 [==============================] - 7s 339us/step - loss: 5.7300 - categorical_accuracy: 0.3815 - val_loss: 6.0843 - val_categorical_accuracy: 0.3825\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 6.06146\n",
            "Epoch 14/20\n",
            "20288/20288 [==============================] - 7s 338us/step - loss: 5.7229 - categorical_accuracy: 0.3815 - val_loss: 6.0938 - val_categorical_accuracy: 0.3825\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 6.06146\n",
            "Epoch 15/20\n",
            "20288/20288 [==============================] - 7s 326us/step - loss: 5.7180 - categorical_accuracy: 0.3815 - val_loss: 6.0686 - val_categorical_accuracy: 0.3825\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 6.06146\n",
            "Epoch 16/20\n",
            "20288/20288 [==============================] - 7s 325us/step - loss: 5.7116 - categorical_accuracy: 0.3815 - val_loss: 6.1436 - val_categorical_accuracy: 0.3825\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 6.06146\n",
            "Epoch 17/20\n",
            "20288/20288 [==============================] - 7s 337us/step - loss: 5.7006 - categorical_accuracy: 0.3815 - val_loss: 6.0793 - val_categorical_accuracy: 0.3825\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 6.06146\n",
            "Epoch 18/20\n",
            "20288/20288 [==============================] - 7s 332us/step - loss: 5.6787 - categorical_accuracy: 0.3815 - val_loss: 6.0651 - val_categorical_accuracy: 0.3825\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 6.06146\n",
            "Epoch 19/20\n",
            "20288/20288 [==============================] - 7s 332us/step - loss: 5.6470 - categorical_accuracy: 0.3815 - val_loss: 6.0418 - val_categorical_accuracy: 0.3825\n",
            "\n",
            "Epoch 00019: val_loss improved from 6.06146 to 6.04178, saving model to saved_models/weight.best.from_scratch.hdf5\n",
            "Epoch 20/20\n",
            "20288/20288 [==============================] - 7s 324us/step - loss: 5.6121 - categorical_accuracy: 0.3815 - val_loss: 6.0289 - val_categorical_accuracy: 0.3825\n",
            "\n",
            "Epoch 00020: val_loss improved from 6.04178 to 6.02889, saving model to saved_models/weight.best.from_scratch.hdf5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "65"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rGvw1d3YmeYf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ab8673c7-d2ca-4cd2-8f6a-092dc5c50374"
      },
      "source": [
        "model.load_weights('saved_models/weight.best.from_scratch.hdf5')\n",
        "pred = model.predict(x_test_images, verbose=1)\n",
        "print(pred.shape)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2537/2537 [==============================] - 1s 246us/step\n",
            "(2537, 5005)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3JuQhrdf9wa8"
      },
      "source": [
        "## MAP@5 for Base CNN Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_RTx0Sfs37Gk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "340627a3-8577-4266-c79a-30b20d40ee89"
      },
      "source": [
        "def map5_per_image(label, predictions):\n",
        "    try:\n",
        "#         print(label,predictions)\n",
        "        return 1 / (predictions[:5].index(label) + 1)\n",
        "    except ValueError:\n",
        "        return 0.0\n",
        "\n",
        "\n",
        "def map5_per_set(labels, predictions):\n",
        "    a = [map5_per_image(l, p) for l, p in zip(labels, predictions)]\n",
        "    print(a)\n",
        "    return np.mean([map5_per_image(l, p) for l, p in zip(labels, predictions)])\n",
        "\n",
        "predictions=[]\n",
        "for i, p in enumerate(pred):\n",
        "    predictions.append(le.inverse_transform(p.argsort()[-5:][::-1]).tolist())\n",
        "print(X_train.Id.__class__)\n",
        "print('MAP@5 score for Base model = %.5f' %(map5_per_set(X_test.Id.tolist(), predictions )))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.series.Series'>\n",
            "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.25, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 1.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.25, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.3333333333333333, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.25, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 1.0, 1.0, 1.0, 1.0, 0.0, 0.25, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.25, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.2, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.3333333333333333, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.25, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.25, 0.5, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.2, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.3333333333333333, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.2, 1.0, 0.0, 0.0, 1.0, 0.0, 0.3333333333333333, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.3333333333333333, 0.0, 0.3333333333333333, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.2, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.2, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.3333333333333333, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.3333333333333333, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.3333333333333333, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.3333333333333333, 1.0, 1.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3333333333333333, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.3333333333333333, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.3333333333333333, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.2, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.25, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.2, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 0.0, 0.25, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.2, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.2, 1.0, 1.0, 0.0, 0.25, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.2, 0.0, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.25, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.2, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.25, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.25, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.3333333333333333, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.25, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.2, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.3333333333333333, 0.3333333333333333, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0]\n",
            "MAP@5 score for Base model = 0.38980\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yYw1BWYVJiZi"
      },
      "source": [
        "# Using VGG16 and Transfer learning\n",
        "\n",
        "To reduce training time without sacrificing accuracy, lets train a CNN using transfer learning.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ju48Lz0tiWK-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_steps_size(generator, batch_size):\n",
        "    nb_samples = len(generator.filenames) \n",
        "    return int(math.ceil(nb_samples / batch_size))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wulZQnv2KHfN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "47203382-2209-4d13-dcd9-7b9404537c16"
      },
      "source": [
        "vgg_model = VGG19(include_top=False, weights='imagenet')\n",
        "\n",
        "train_datagen = image.ImageDataGenerator(\n",
        "    rescale=1./255)\n",
        "batch_size=100\n",
        "\n",
        "generator = train_datagen.flow_from_dataframe(dataframe=X_train,\n",
        "                                              directory='input/train',\n",
        "                                              x_col=\"Image\",\n",
        "                                              y_col=\"Id\", \n",
        "                                              classes=le.classes_.tolist(),\n",
        "                                              target_size=(image_height, image_width),\n",
        "                                              batch_size=batch_size, shuffle=False, drop_duplicates=False) \n",
        "\n",
        "\n",
        "train_bc_features = vgg_model.predict_generator(generator, steps=get_steps_size(generator, batch_size),verbose=1)\n",
        "\n",
        "test_datagen = image.ImageDataGenerator(rescale=1./255)\n",
        "generator = test_datagen.flow_from_dataframe(dataframe=X_val,\n",
        "                                              directory='input/train',\n",
        "                                              x_col=\"Image\",\n",
        "                                              y_col=\"Id\", \n",
        "                                              classes=le.classes_.tolist(),\n",
        "                                              target_size=(image_height, image_width),\n",
        "                                              batch_size=batch_size,\n",
        "                                              shuffle=False,\n",
        "                                              drop_duplicates=False)  \n",
        "val_bc_features = vgg_model.predict_generator(generator, steps=get_steps_size(generator, batch_size),verbose=1)\n",
        "\n",
        "generator = test_datagen.flow_from_dataframe(dataframe=X_test,\n",
        "                                              directory='input/train',\n",
        "                                              x_col=\"Image\",\n",
        "                                              y_col=\"Id\", \n",
        "                                              classes=le.classes_.tolist(),\n",
        "                                              target_size=(image_height, image_width),\n",
        "                                              batch_size=batch_size,\n",
        "                                              shuffle=False,\n",
        "                                              drop_duplicates=False)  \n",
        "test_bc_features = vgg_model.predict_generator(generator, steps=get_steps_size(generator, batch_size),verbose=1)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "80142336/80134624 [==============================] - 1s 0us/step\n",
            "Found 20288 validated image filenames belonging to 5005 classes.\n",
            "203/203 [==============================] - 161s 793ms/step\n",
            "Found 2536 validated image filenames belonging to 5005 classes.\n",
            "26/26 [==============================] - 20s 767ms/step\n",
            "Found 2537 validated image filenames belonging to 5005 classes.\n",
            "26/26 [==============================] - 20s 777ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xd_-LCCaiWLB",
        "colab_type": "text"
      },
      "source": [
        "## Build the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M72zSwMXiWLC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "8325e8d3-e14c-440c-c273-4b426f49cf32"
      },
      "source": [
        "print(train_bc_features.shape)\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Flatten(input_shape = train_bc_features.shape[1:]))\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(20288, 3, 3, 512)\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_5 (Flatten)          (None, 4608)              0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 512)               2359808   \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 5005)              2567565   \n",
            "=================================================================\n",
            "Total params: 4,927,373\n",
            "Trainable params: 4,927,373\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uucB5i_qiWLE",
        "colab_type": "text"
      },
      "source": [
        "## Compile the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qks3sBgIiWLE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.metrics import top_k_categorical_accuracy,categorical_crossentropy\n",
        "\n",
        "def top_5_accuracy(y_true, y_pred):\n",
        "    return top_k_categorical_accuracy(y_true, y_pred, k=5)\n",
        "\n",
        "model.compile(optimizer='rmsprop',  \n",
        "              loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLuB0kS-iWLG",
        "colab_type": "text"
      },
      "source": [
        "## Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-qWon4jiWLG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "80e67fde-1d98-41f9-bdce-fb6682919716"
      },
      "source": [
        "checkpointer = ModelCheckpoint(filepath='saved_models/bottleneck_fc_model.hdf5',\n",
        "                               verbose=1, save_best_only = True)\n",
        "\n",
        "history = model.fit(train_bc_features, Y_train,  \n",
        "          epochs=40,  \n",
        "          batch_size=batch_size,  \n",
        "          validation_data=(val_bc_features, Y_val), callbacks=[checkpointer])  \n",
        "   \n"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 20288 samples, validate on 2536 samples\n",
            "Epoch 1/40\n",
            "20288/20288 [==============================] - 6s 295us/step - loss: 6.2533 - acc: 0.3782 - val_loss: 5.8693 - val_acc: 0.3825\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 5.86927, saving model to saved_models/bottleneck_fc_model.hdf5\n",
            "Epoch 2/40\n",
            "20288/20288 [==============================] - 2s 109us/step - loss: 6.0681 - acc: 0.3815 - val_loss: 5.7702 - val_acc: 0.3825\n",
            "\n",
            "Epoch 00002: val_loss improved from 5.86927 to 5.77023, saving model to saved_models/bottleneck_fc_model.hdf5\n",
            "Epoch 3/40\n",
            "20288/20288 [==============================] - 2s 106us/step - loss: 5.9727 - acc: 0.3815 - val_loss: 5.8215 - val_acc: 0.3825\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 5.77023\n",
            "Epoch 4/40\n",
            "20288/20288 [==============================] - 2s 106us/step - loss: 5.9275 - acc: 0.3815 - val_loss: 5.8443 - val_acc: 0.3825\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 5.77023\n",
            "Epoch 5/40\n",
            "20288/20288 [==============================] - 2s 106us/step - loss: 5.8564 - acc: 0.3815 - val_loss: 5.8371 - val_acc: 0.3825\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 5.77023\n",
            "Epoch 6/40\n",
            "20288/20288 [==============================] - 2s 104us/step - loss: 5.8477 - acc: 0.3815 - val_loss: 6.0323 - val_acc: 0.3825\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 5.77023\n",
            "Epoch 7/40\n",
            "20288/20288 [==============================] - 2s 105us/step - loss: 5.7932 - acc: 0.3815 - val_loss: 5.8391 - val_acc: 0.3825\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 5.77023\n",
            "Epoch 8/40\n",
            "20288/20288 [==============================] - 2s 107us/step - loss: 5.7652 - acc: 0.3815 - val_loss: 5.8610 - val_acc: 0.3825\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 5.77023\n",
            "Epoch 9/40\n",
            "20288/20288 [==============================] - 2s 106us/step - loss: 5.7333 - acc: 0.3815 - val_loss: 5.7706 - val_acc: 0.3825\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 5.77023\n",
            "Epoch 10/40\n",
            "20288/20288 [==============================] - 2s 105us/step - loss: 5.7062 - acc: 0.3815 - val_loss: 6.1167 - val_acc: 0.3825\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 5.77023\n",
            "Epoch 11/40\n",
            "20288/20288 [==============================] - 2s 105us/step - loss: 5.6811 - acc: 0.3815 - val_loss: 6.0629 - val_acc: 0.3825\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 5.77023\n",
            "Epoch 12/40\n",
            "20288/20288 [==============================] - 2s 106us/step - loss: 5.6563 - acc: 0.3815 - val_loss: 5.8816 - val_acc: 0.3825\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 5.77023\n",
            "Epoch 13/40\n",
            "20288/20288 [==============================] - 2s 107us/step - loss: 5.6338 - acc: 0.3815 - val_loss: 6.1887 - val_acc: 0.3825\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 5.77023\n",
            "Epoch 14/40\n",
            "20288/20288 [==============================] - 2s 106us/step - loss: 5.6196 - acc: 0.3815 - val_loss: 5.9406 - val_acc: 0.3825\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 5.77023\n",
            "Epoch 15/40\n",
            "20288/20288 [==============================] - 2s 106us/step - loss: 5.6328 - acc: 0.3815 - val_loss: 6.4990 - val_acc: 0.3825\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 5.77023\n",
            "Epoch 16/40\n",
            "20288/20288 [==============================] - 2s 106us/step - loss: 5.6061 - acc: 0.3815 - val_loss: 5.8734 - val_acc: 0.3825\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 5.77023\n",
            "Epoch 17/40\n",
            "20288/20288 [==============================] - 2s 106us/step - loss: 5.6074 - acc: 0.3815 - val_loss: 6.0089 - val_acc: 0.3825\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 5.77023\n",
            "Epoch 18/40\n",
            "20288/20288 [==============================] - 2s 101us/step - loss: 5.5877 - acc: 0.3815 - val_loss: 5.9197 - val_acc: 0.3825\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 5.77023\n",
            "Epoch 19/40\n",
            "20288/20288 [==============================] - 2s 104us/step - loss: 5.5720 - acc: 0.3815 - val_loss: 6.2207 - val_acc: 0.3825\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 5.77023\n",
            "Epoch 20/40\n",
            "20288/20288 [==============================] - 2s 106us/step - loss: 5.5631 - acc: 0.3815 - val_loss: 5.9884 - val_acc: 0.3825\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 5.77023\n",
            "Epoch 21/40\n",
            "20288/20288 [==============================] - 2s 106us/step - loss: 5.5604 - acc: 0.3815 - val_loss: 6.6891 - val_acc: 0.3825\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 5.77023\n",
            "Epoch 22/40\n",
            "20288/20288 [==============================] - 2s 104us/step - loss: 5.5459 - acc: 0.3814 - val_loss: 5.9733 - val_acc: 0.3825\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 5.77023\n",
            "Epoch 23/40\n",
            "20288/20288 [==============================] - 2s 106us/step - loss: 5.5386 - acc: 0.3815 - val_loss: 6.2724 - val_acc: 0.3825\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 5.77023\n",
            "Epoch 24/40\n",
            "20288/20288 [==============================] - 2s 106us/step - loss: 5.5649 - acc: 0.3815 - val_loss: 6.0132 - val_acc: 0.3825\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 5.77023\n",
            "Epoch 25/40\n",
            "20288/20288 [==============================] - 2s 104us/step - loss: 5.5337 - acc: 0.3815 - val_loss: 5.9936 - val_acc: 0.3825\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 5.77023\n",
            "Epoch 26/40\n",
            "20288/20288 [==============================] - 2s 107us/step - loss: 5.5399 - acc: 0.3815 - val_loss: 6.2552 - val_acc: 0.3825\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 5.77023\n",
            "Epoch 27/40\n",
            "20288/20288 [==============================] - 2s 106us/step - loss: 5.5423 - acc: 0.3814 - val_loss: 6.1389 - val_acc: 0.3825\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 5.77023\n",
            "Epoch 28/40\n",
            "20288/20288 [==============================] - 2s 106us/step - loss: 5.5376 - acc: 0.3815 - val_loss: 6.0424 - val_acc: 0.3825\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 5.77023\n",
            "Epoch 29/40\n",
            "20288/20288 [==============================] - 2s 106us/step - loss: 5.5006 - acc: 0.3815 - val_loss: 6.1774 - val_acc: 0.3825\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 5.77023\n",
            "Epoch 30/40\n",
            "20288/20288 [==============================] - 2s 106us/step - loss: 5.5146 - acc: 0.3816 - val_loss: 6.1397 - val_acc: 0.3825\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 5.77023\n",
            "Epoch 31/40\n",
            "20288/20288 [==============================] - 2s 106us/step - loss: 5.5144 - acc: 0.3816 - val_loss: 5.9669 - val_acc: 0.3825\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 5.77023\n",
            "Epoch 32/40\n",
            "20288/20288 [==============================] - 2s 107us/step - loss: 5.5545 - acc: 0.3813 - val_loss: 6.0694 - val_acc: 0.3825\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 5.77023\n",
            "Epoch 33/40\n",
            "20288/20288 [==============================] - 2s 107us/step - loss: 5.5486 - acc: 0.3814 - val_loss: 6.0571 - val_acc: 0.3825\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 5.77023\n",
            "Epoch 34/40\n",
            "20288/20288 [==============================] - 2s 106us/step - loss: 5.5242 - acc: 0.3816 - val_loss: 6.1216 - val_acc: 0.3825\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 5.77023\n",
            "Epoch 35/40\n",
            "20288/20288 [==============================] - 2s 106us/step - loss: 5.5190 - acc: 0.3817 - val_loss: 6.0651 - val_acc: 0.3825\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 5.77023\n",
            "Epoch 36/40\n",
            "20288/20288 [==============================] - 2s 104us/step - loss: 5.4977 - acc: 0.3817 - val_loss: 6.0282 - val_acc: 0.3825\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 5.77023\n",
            "Epoch 37/40\n",
            "20288/20288 [==============================] - 2s 105us/step - loss: 5.5315 - acc: 0.3817 - val_loss: 6.0547 - val_acc: 0.3825\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 5.77023\n",
            "Epoch 38/40\n",
            "20288/20288 [==============================] - 2s 105us/step - loss: 5.5276 - acc: 0.3814 - val_loss: 6.0126 - val_acc: 0.3825\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 5.77023\n",
            "Epoch 39/40\n",
            "20288/20288 [==============================] - 2s 105us/step - loss: 5.5211 - acc: 0.3814 - val_loss: 6.2524 - val_acc: 0.3825\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 5.77023\n",
            "Epoch 40/40\n",
            "20288/20288 [==============================] - 2s 106us/step - loss: 5.5311 - acc: 0.3818 - val_loss: 6.0793 - val_acc: 0.3825\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 5.77023\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xe3s4Ld7Q5tj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "f57fbd09-712a-4129-b3a7-9ec2a8d251a0"
      },
      "source": [
        "(eval_loss, eval_accuracy) = model.evaluate(val_bc_features, Y_val, batch_size=batch_size, verbose=1)\n",
        "\n",
        "print(\"[INFO] accuracy: {:.2f}%\".format(eval_accuracy * 100))  \n",
        "print(\"[INFO] Loss: {}\".format(eval_loss))  "
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2536/2536 [==============================] - 0s 71us/step\n",
            "[INFO] accuracy: 38.25%\n",
            "[INFO] Loss: 6.07927381653891\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjwtYqj8iWLI",
        "colab_type": "text"
      },
      "source": [
        "## Test the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDE9EOPIiWLI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3606e8a1-b2c7-4f88-f509-8b85fa459f3a"
      },
      "source": [
        "model.load_weights('saved_models/bottleneck_fc_model.hdf5')\n",
        "pred = model.predict(test_bc_features, verbose=1)\n",
        "\n",
        "predictions=[]\n",
        "for i, p in enumerate(pred):\n",
        "    predictions.append(le.inverse_transform(p.argsort()[-5:][::-1]).tolist())\n",
        "print('MAP@5 score for VGG16 model = %.5f' %(map5_per_set(X_test.Id, predictions )))\n",
        "print(X_test.Id[1::])"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2537/2537 [==============================] - 2s 626us/step\n",
            "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.2, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.2, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.2, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.3333333333333333, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.25, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.2, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.25, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3333333333333333, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.25, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.2, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.2, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.25, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.25, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0]\n",
            "MAP@5 score for VGG16 model = 0.37933\n",
            "21874    w_f382f59\n",
            "10111    w_0bf90d9\n",
            "5302     w_4516d31\n",
            "24115    w_76065ea\n",
            "18120    w_91037a7\n",
            "4790     new_whale\n",
            "4880     w_276d61d\n",
            "21402    new_whale\n",
            "24333    w_076bb5a\n",
            "7752     w_27597ff\n",
            "5579     w_ac80688\n",
            "10144    w_c1bab16\n",
            "24856    new_whale\n",
            "18133    w_d236b51\n",
            "1783     new_whale\n",
            "1343     w_00904a7\n",
            "20184    w_acfc8c1\n",
            "21854    w_9353e7c\n",
            "8147     new_whale\n",
            "23370    w_8799716\n",
            "18685    w_46ea040\n",
            "2608     w_fd3e556\n",
            "16911    new_whale\n",
            "5838     w_090c801\n",
            "20511    w_46ba1ce\n",
            "23685    w_b8e1687\n",
            "2647     w_1af6b9a\n",
            "19256    w_20950a9\n",
            "17053    w_df32eec\n",
            "4543     w_0fdf741\n",
            "           ...    \n",
            "10493    new_whale\n",
            "3927     w_27a6304\n",
            "14446    new_whale\n",
            "1103     w_e21b629\n",
            "24297    new_whale\n",
            "6018     w_98d094e\n",
            "6494     new_whale\n",
            "20818    w_2e374c0\n",
            "8409     w_b475feb\n",
            "6009     w_92a47eb\n",
            "15184    w_b7cf3f5\n",
            "12857    w_3815890\n",
            "11014    w_6df47c6\n",
            "21645    new_whale\n",
            "15229    w_ac80688\n",
            "6606     w_8bc8a32\n",
            "93       new_whale\n",
            "12718    new_whale\n",
            "22798    w_81f8d47\n",
            "8441     w_60ce6fc\n",
            "15318    w_6caff0f\n",
            "4627     w_90abfeb\n",
            "25305    w_9af432a\n",
            "16851    new_whale\n",
            "22697    w_bc28a13\n",
            "1647     w_9142b9c\n",
            "12728    new_whale\n",
            "6644     w_107b98e\n",
            "1532     new_whale\n",
            "14675    new_whale\n",
            "Name: Id, Length: 2536, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kEhcIrAiWLK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}